{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Argument_assessment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYl7X4Elha4p"
      },
      "source": [
        "# Paths to data files and output file on drive\n",
        "train_data_file = '/content/drive/MyDrive/train-data-prepared.json'\n",
        "val_data_file = '/content/drive/MyDrive/val-data-prepared.json' #needs to be modified for test file\n",
        "pred_out_file = '/content/drive/MyDrive/prediction_out.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKRgCtldjF17"
      },
      "source": [
        "# Import all dependencies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import nltk\n",
        "import json\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,LSTM,Embedding,InputLayer,Concatenate\n",
        "from tensorflow.keras import Model\n",
        "from sklearn import metrics\n",
        "import keras\n",
        "\n",
        "#Spacy english model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-pRx3tZu84V"
      },
      "source": [
        "# Function to Preprocess the text data: Lower Casing, Remove URLs, punctuations and stripping extra spaces\n",
        "def PreprocessData(df):\n",
        "  df['clean_text'] = df['_body'].str.lower()\n",
        "  df['clean_text'] = df['clean_text'].str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ') #remove URL\n",
        "  df['clean_text'] = df['clean_text'].str.strip()\n",
        "  df['clean_text'] = df['clean_text'].str.replace('[^\\w\\s]','')\n",
        "  return df\n",
        "\n",
        "# Function to concatenate posts of same ID\n",
        "def ConcatenatePost(df):\n",
        "  df['clean_text_copy'] = df.groupby(['id'])['clean_text'].transform(lambda x : ' '.join(x))\n",
        "  df['clean_text'] = df.groupby(['id'])['clean_text'].transform(lambda x : '||'.join(x))\n",
        "  df['_body'] = df.groupby(['id'])['_body'].transform(lambda x : '|'.join(x))\n",
        "  df['label'] = df.groupby(['id'])['label'].transform('max')\n",
        "  df = df.drop_duplicates(inplace=False)\n",
        "  return df\n",
        "\n",
        "# Function to extract lexical features from text\n",
        "def FeatureExtract(df):\n",
        "\n",
        "  offensive_words = [\"ass\",\"idiot\",\"moron\",\"stupid\",\"bitch\",\"shit\",\"fuck\",\"dumb\",\"fool\",\"pussy\"]\n",
        "  advmod_exist_list = [] #intensifiers eg: absolutely, very, extremely, seriously etc\n",
        "  prp_exist_list = [] #personal pronouns eg: I, me, you etc\n",
        "  acomp_exist_list = [] #adjectivial complement eg: unreal, unsatisfactory, unwilling\n",
        "  relcl_exist_list = [] #eg: hurts, torutres, celebrates\n",
        "  abuse_exist_list = [] #bad and offensive words\n",
        "  for text in df['clean_text_copy']:\n",
        "    doc = nlp(text)\n",
        "    advmod_exist = 0\n",
        "    prp_exist = 0\n",
        "    acomp_exist = 0\n",
        "    relcl_exist = 0\n",
        "    abuse_exist = 0\n",
        "    for token in doc:\n",
        "      if token.dep_ == 'advmod':\n",
        "        advmod_exist = advmod_exist + 1\n",
        "      \n",
        "      if token.tag_ == 'PRP':\n",
        "        prp_exist = prp_exist + 1\n",
        "      \n",
        "      if token.pos_ == 'ADJ' and token.dep_ == 'acomp':\n",
        "        acomp_exist = acomp_exist + 1\n",
        "      \n",
        "      if token.text in offensive_words:\n",
        "        abuse_exist = abuse_exist + 1\n",
        "      \n",
        "      if token.dep_ == 'relcl':\n",
        "        relcl_exist = relcl_exist + 1\n",
        "    \n",
        "    advmod_exist_list.append(advmod_exist)\n",
        "    prp_exist_list.append(prp_exist)\n",
        "    acomp_exist_list.append(acomp_exist)\n",
        "    abuse_exist_list.append(abuse_exist)\n",
        "    relcl_exist_list.append(relcl_exist)\n",
        "  \n",
        "  df['intensifier'] = advmod_exist_list\n",
        "  df['prp'] = prp_exist_list\n",
        "  df['acomp'] = acomp_exist_list\n",
        "  df['abuse'] = abuse_exist_list\n",
        "  df['relcl'] = relcl_exist_list\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNWk-REDjF18",
        "outputId": "05cf524c-a729-47c6-afd8-c3b9963e31f9"
      },
      "source": [
        "# Read the train and validation dataset\n",
        "with open(train_data_file, 'r') as f:\n",
        "  json_data_train = json.load(f)\n",
        "\n",
        "with open(val_data_file, 'r') as f:\n",
        "  json_data_val = json.load(f)\n",
        "\n",
        "# Normalizing the nested structure\n",
        "init_data_train = pd.json_normalize(json_data_train, record_path='preceding_posts',meta= ['id','label'],max_level=1, record_prefix='_')\n",
        "init_data_val = pd.json_normalize(json_data_val, record_path='preceding_posts',meta= ['id','label'],max_level=1, record_prefix='_')\n",
        "\n",
        "# Selecting data on interest\n",
        "data_train = init_data_train[['id','_body','label']]\n",
        "data_val = init_data_val[['id','_body','label']]\n",
        "\n",
        "# Function call to preprocess data\n",
        "df_train = PreprocessData(data_train)\n",
        "df_val = PreprocessData(data_val)\n",
        "\n",
        "# Function call to concatenate posts of same ID\n",
        "df_train = ConcatenatePost(df_train)\n",
        "df_val = ConcatenatePost(df_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRrlybB3MMa_",
        "outputId": "ae73f31b-d21b-46de-bfda-45da303b7c86"
      },
      "source": [
        "# Extract lexical features\n",
        "df_train = FeatureExtract(df_train)\n",
        "df_val = FeatureExtract(df_val)\n",
        "\n",
        "# creating feature matrix of train and val dataset\n",
        "x_train_text_features = df_train[[\"intensifier\",\"prp\",\"acomp\",\"abuse\",\"relcl\"]].values\n",
        "x_val_text_features = df_val[[\"intensifier\",\"prp\",\"acomp\",\"abuse\",\"relcl\"]].values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:57: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzsHUeCzqGIe"
      },
      "source": [
        "# Tokenizer object to generate integer values for a vocabulary of words. This is prerequise for generating embeddings using keras\n",
        "\n",
        "max_vocab = 2500\n",
        "tokenizer = Tokenizer(num_words = max_vocab,split = ' ', oov_token=1 )\n",
        "\n",
        "# Fit the tokenizer and transform tokens to integer sequence\n",
        "tokenizer.fit_on_texts(df_train['clean_text_copy'].values)\n",
        "x_train_neural =  tokenizer.texts_to_sequences(df_train['clean_text_copy'].values)\n",
        "x_val_neural = tokenizer.texts_to_sequences(df_val['clean_text_copy'])\n",
        "\n",
        "# Pad the sequence to max length for uniformity of matrix\n",
        "max_length = max(len(s.split()) for s in df_train['clean_text_copy'].values)\n",
        "x_train_neural = pad_sequences(x_train_neural,maxlen=max_length)\n",
        "x_val_neural = pad_sequences(x_val_neural,maxlen=max_length)\n",
        "\n",
        "y_train = df_train['label'].values\n",
        "y_val = df_val['label'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExBVN66vRrjA"
      },
      "source": [
        "# Neural Network for Classification\n",
        "\n",
        "embed_dim = 32\n",
        "lstm_out = 50\n",
        "\n",
        "# Model 1: Embeddibng layer to generate embedding and LSTM network for sequential modelling. Input- word tokenized matrix\n",
        "model1 = Sequential()\n",
        "model1.add(Embedding(max_vocab,embed_dim,input_length = x_train_neural.shape[1]))\n",
        "model1.add(LSTM(lstm_out,dropout = 0.2,recurrent_dropout = 0.2))\n",
        "\n",
        "# Model 2: Feed-forward network with 1 hidden layer. Input- Lexical feature matrix\n",
        "model2 = Sequential()\n",
        "model2.add(Dense(10,activation = 'relu',input_shape = (5,)))\n",
        "\n",
        "# Merge Model 1 and Model 2. Merged model has 1 hidden layer.\n",
        "merged_model = Concatenate()([model1.output,model2.output])\n",
        "z = Dense(10,activation = 'relu')(merged_model)\n",
        "z = Dense(1,activation = 'sigmoid')(z)\n",
        "\n",
        "model = Model(inputs=[model1.input, model2.input], outputs=z)\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t-0XGlmSS1a",
        "outputId": "bbc1b754-2693-4475-f909-ece64f5ebc46"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "embedding_input (InputLayer)    [(None, 3033)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 3033, 32)     80000       embedding_input[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_input (InputLayer)        [(None, 5)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 50)           16600       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           60          dense_input[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 60)           0           lstm[0][0]                       \n",
            "                                                                 dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           610         concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            11          dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 97,281\n",
            "Trainable params: 97,281\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLJqZ7GfBWrg",
        "outputId": "e2cf9843-7b4a-4b45-800c-63f829f6b574"
      },
      "source": [
        "# Fitting the neural network\n",
        "model.fit([x_train_neural,x_train_text_features], y_train, epochs = 7, batch_size = 64, verbose = 2,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "31/31 - 136s - loss: 0.8847 - accuracy: 0.4747\n",
            "Epoch 2/7\n",
            "31/31 - 132s - loss: 0.7100 - accuracy: 0.5114\n",
            "Epoch 3/7\n",
            "31/31 - 131s - loss: 0.6727 - accuracy: 0.5992\n",
            "Epoch 4/7\n",
            "31/31 - 131s - loss: 0.5749 - accuracy: 0.7252\n",
            "Epoch 5/7\n",
            "31/31 - 131s - loss: 0.4360 - accuracy: 0.8073\n",
            "Epoch 6/7\n",
            "31/31 - 133s - loss: 0.3251 - accuracy: 0.8709\n",
            "Epoch 7/7\n",
            "31/31 - 134s - loss: 0.2588 - accuracy: 0.8972\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f111d277550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0w8QF44hDOs2"
      },
      "source": [
        "# prediciting the likelihoods and generating predictions using threshhold\n",
        "\n",
        "likelihoods = model.predict([x_val_neural,x_val_text_features])\n",
        "predictions = np.where(likelihoods < 0.5, 0, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GHI3BeCa0Ovf"
      },
      "source": [
        "# Generate dictionary of ID and Prediction and save it to a file\n",
        "\n",
        "val_data_id = df_val['id'].values\n",
        "predictions_final = predictions.flatten().tolist()\n",
        "pred_val = dict(zip(val_data_id, predictions_final))\n",
        "\n",
        "with open(pred_out_file, 'w') as fp:\n",
        "    json.dump(pred_val,fp)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}