{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTRd_NhhOy6L"
   },
   "outputs": [],
   "source": [
    "# Paths to data files and output file on drive\n",
    "train_data_file = 'train_data.json'\n",
    "val_data_file = 'valid_data.json' #needs to be modified for test file\n",
    "pred_out_file = 'prediction_out.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dz3sRO0_X9If"
   },
   "outputs": [],
   "source": [
    "# Import all dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import json\n",
    "# from transformers import PegasusTokenizer, TFPegasusForConditionalGeneration\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "\n",
    "# model = TFPegasusForConditionalGeneration.from_pretrained('google/pegasus-xsum')\n",
    "# tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-xsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "ZLLJULWBYA55",
    "outputId": "8f69c523-4874-4c0e-a590-7a8f1a471033"
   },
   "outputs": [],
   "source": [
    "df_train_init = pd.read_json(train_data_file)\n",
    "df_val_init = pd.read_json(val_data_file)\n",
    "\n",
    "display(df_val_init.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l816hMk5Zh7x"
   },
   "outputs": [],
   "source": [
    "# df_train_clean = PreprocessData(df_train_init)\n",
    "# df_val_clean = PreprocessData(df_val_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE = df_val_init['argument'].iloc[5]\n",
    "# print(ARTICLE)\n",
    "inputs = tokenizer.encode(\"summarize: \" + ARTICLE, return_tensors=\"tf\", max_length=512,truncation = True)\n",
    "outputs = model.generate(inputs, max_length=80, min_length=10, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "print(df_val_init['conclusion'].iloc[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARTICLE = df_val_init['argument'].iloc[2]\n",
    "# print(ARTICLE)\n",
    "\n",
    "# print(df_val_init['conclusion'].iloc[5])\n",
    "gen_conc = str(tokenizer.decode(outputs[0]))\n",
    "gen_conc = gen_conc.replace(\"<pad>\",\"\").lstrip().rstrip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conclusion_generate(argument):\n",
    "    \n",
    "    inputs = tokenizer.encode(\"summarize: \" + argument, return_tensors = \"tf\", max_length = 512, truncation = True)\n",
    "    outputs = model.generate(inputs, max_length=150, min_length=10, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    gen_conclusion = str(tokenizer.decode(outputs[0]))\n",
    "    gen_conclusion = gen_conclusion.replace(\"<pad>\",\"\").lstrip().rstrip()\n",
    "    \n",
    "    return gen_conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_init['generated_conc'] = df_val_init['argument'].apply(lambda x: conclusion_generate(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OB0lCvcgpPfP"
   },
   "outputs": [],
   "source": [
    "# # Contraction mapping dictionary obtained from internet\n",
    "\n",
    "# contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "#                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "#                            \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "#                            \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "#                            \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "#                            \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "#                            \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "#                            \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "#                            \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "#                            \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "#                            \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "#                            \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "#                            \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "#                            \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "#                            \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "#                            \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "#                            \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "#                            \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "#                            \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "#                            \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "#                            \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "#                            \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "#                            \"you're\": \"you are\", \"you've\": \"you have\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFp7cbxUdsW4"
   },
   "outputs": [],
   "source": [
    "# # Function to Preprocess the text data: Lower Casing, Remove URLs, punctuations and stripping extra spaces\n",
    "# def PreprocessData(df):\n",
    "#   for column in ['conclusion','argument']:\n",
    "#       df['clean_' + column] = df[column].str.lower()\n",
    "#       df['clean_' + column] = df['clean_' + column].str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ') #remove URL\n",
    "#       df['clean_' + column] = df['clean_' + column].apply(lambda x: ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in x.split()]))\n",
    "#       df['clean_' + column] = df['clean_' + column].str.strip()\n",
    "#       df['clean_' + column] = df['clean_' + column].str.replace('[^\\w\\s]','')\n",
    "#       df['clean_' + column] = df['clean_' + column].str.replace('\\n','')\n",
    "#       df['clean_' + column] = df['clean_' + column].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords]))\n",
    "#   return df\n",
    "\n",
    "# df['response'] = df['response'].apply(lambda x: [item for item in x.split() if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgO6dOkk83-W"
   },
   "outputs": [],
   "source": [
    "# # Argument tokenizer\n",
    "\n",
    "# # Fit the tokenzier on train data\n",
    "# arg_tokenizer = Tokenizer(oov_token=1)\n",
    "# arg_tokenizer.fit_on_texts(df_train_clean['clean_argument'].values)\n",
    "\n",
    "# # Transform the texts to integer sequences - train and val data\n",
    "# x_train_arg = arg_tokenizer.texts_to_sequences(df_train_clean['clean_argument'].values)\n",
    "# x_val_arg = arg_tokenizer.texts_to_sequences(df_val_clean['clean_argument'].values)\n",
    "\n",
    "# # Pad length for uniformity\n",
    "# max_length = max(len(s.split()) for s in df_train_clean['clean_argument'].values)\n",
    "# x_train_arg = pad_sequences(x_train_arg,maxlen=max_length)\n",
    "# x_val_arg = pad_sequences(x_val_arg,maxlen=max_length)\n",
    "\n",
    "# # Conclusion tokenizer\n",
    "\n",
    "# # Fit the tokenzier on train data\n",
    "# conc_tokenizer = Tokenizer(oov_token=1)\n",
    "# conc_tokenizer.fit_on_texts(df_train_clean['clean_conclusion'].values)\n",
    "\n",
    "# # Transform the texts to integer sequences - train and val data\n",
    "# x_train_conc = conc_tokenizer.texts_to_sequences(df_train_clean['clean_conclusion'].values)\n",
    "# x_val_conc = conc_tokenizer.texts_to_sequences(df_val_clean['clean_conclusion'].values)\n",
    "\n",
    "# # Pad length for uniformity\n",
    "# max_length = max(len(s.split()) for s in df_train_clean['clean_conclusion'].values)\n",
    "# x_train_conc = pad_sequences(x_train_conc,maxlen=max_length)\n",
    "# x_val_conc = pad_sequences(x_val_conc,maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qjtJLASm1k9M"
   },
   "outputs": [],
   "source": [
    "# ARTICLE_TO_SUMMARIZE = (\n",
    "# \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n",
    "# \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n",
    "# \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n",
    "# )\n",
    "# article = df_val_init['argument'].iloc[1]\n",
    "# # Tokenizes the input sequence upto 1024 characters.\n",
    "# inputs = tokenizer([article], max_length=1024, return_tensors='tf',truncation = True)\n",
    "\n",
    "# # Generate Summary\n",
    "# summary_ids = model.generate(inputs['input_ids'])\n",
    "# print([tokenizer.decode(g, clean_up_tokenization_spaces=False) for g in summary_ids])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Argument_generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
